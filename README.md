# Como Reduzi 92% do desperd√≠cio no Cluster Kubernetes

## O Problema:

Tudo come√ßou quando um dev chegou at√© a mim e me perguntou: "Marcelo, nosso cluster AMP (ApplicaApplication Management Platform) tem espa√ßo suficiente para subirmos dois ambientes: develop e homolog?"

Essa pergunta me fez refletir muito sobre utiliza√ß√£o de recursos. Eu j√° sabia que t√≠nhamos recursos dispon√≠veis, mas... quanto de recurso exatamente temos? Qual o consumo efetivo de cada aplica√ß√£o? Decidi ent√£o fazer algo que ainda n√£o tinha sido feito antes: auditar de verdade o que est√°vamos usando versus o que est√°vamos reservando no cluster.

Utilizamos Prometheus, analisei os dados e m√©tricas, rodei alguns scripts que desenvolvi e... descobri um baita de um problema.

Nosso cluster Kubernetes (RKE2, com 3 nodes) estava solicitando **13,5 CPUs** mas efetivamente utilizando apenas **1,5 CPUs**. A efici√™ncia era de m√≠seros **10%**. Ou seja, est√°vamos desperdi√ßando **quase 90% dos recursos**.

Mas ent√£o como chegamos nessa realidade? Configura√ß√µes gen√©ricas copiadas de exemplos, aquele famoso "vamos colocar 500m pra garantir", e ningu√©m nunca revisou depois. O resultado: Um grande desperd√≠cio.

Decidi criar em reposit√≥rio para documentar todo o processo que segui para diagnosticar, analisar e otimizar o cluster - resultando em **economia significativa** sem causar **nenhum problema** nas aplica√ß√µes.

---

## O qu√™ eu queria Alcan√ßar

Meu objetivo: otimizar os recursos do cluster sem causar nenhum problema. Isso significava:

- Reduzir o desperd√≠cio de CPU e Mem√≥ria
- Melhorar a efici√™ncia do scheduler do Kubernetes
- Liberar recursos para novas aplica√ß√µes
- Fazer tudo isso mantendo o SLA e a performance das aplica√ß√µes
- Zero downtime

Basicamente, queria usar os recursos de forma inteligente, n√£o apenas simplesmente cortar.

---

## Resultados alcan√ßados

### Situa√ß√£o do Cluster (Antes da Otimiza√ß√£o)

| M√©trica | Valor |
|---------|-------|
| **CPU Total Solicitada** | 13.535m (~13.5 CPUs) |
| **CPU Total Utilizada** | 1.535m (~1.5 CPUs) |
| **Desperd√≠cio M√©dio** | **88.6%** üî¥ |
| **Efici√™ncia** | 11.4% |

### Primeiro Caso: Namespace Velero (Como piloto)

Decidi come√ßar pelo Velero (nossa ferramenta de backup) porque:
1. N√£o era cr√≠tico para o neg√≥cio (backups rodavam de madrugada)
2. Mostrava o maior percentual de desperd√≠cio (98.9%)
3. Se desse algum problema, seria f√°cil reverter

Os n√∫meros antes da otimiza√ß√£o:

| M√©trica | Antes | Depois | Melhoria |
|---------|-------|--------|----------|
| CPU Solicitada | 560m | 45m | **-92%** ‚úÖ |
| CPU Utilizada | ~6m | ~6m | Sem impacto |
| Desperd√≠cio | 98.9% | 86.7% | **-12.2pp** |
| Pods Afetados | 4 | 4 | 0 downtime |

**Resultado:** Liberados **515 millicores** de CPU mantendo **margem de seguran√ßa de 7x** o uso real.

---

## Como de fato eu fiz (Metodologia)

### 1. Diagn√≥stico - Descobrindo onde estava o Problema

Primeiro, precisava de dados. Desenvolvi scripts que fazem uma auditoria automatizada do cluster:

```bash
#!/bin/bash
# Script de auditoria que calcula: (CPU Requested) - (CPU Used)
# Output: Ranking de namespaces por percentual de desperd√≠cio

for ns in $(kubectl get ns -o jsonpath='{.items[*].metadata.name}'); do
    # Calcula requests configurados
    REQ=$(kubectl get pods -n "$ns" -o json | jq -r '...')
    
    # Coleta uso real via metrics-server
    USE=$(kubectl top pods -n "$ns" --no-headers | awk '...')
    
    # Calcula slack e percentual
    SLACK=$((REQ - USE))
    PERCENT=$(awk "BEGIN {printf \"%.1f\", ($SLACK / $REQ) * 100}")
done
```

O script me retornou esta lista (ordenada do pior para o melhor):

```
NAMESPACE                    REQUESTED   USED    SLACK    WASTE %
velero                       560m        6m      554m     98.9%  üî¥
istio-system                 1410m       21m     1389m    98.5%  üî¥
cattle-monitoring-system     950m        111m    839m     88.3%  üü°
kube-system                  3925m       625m    3300m    84.1%  üü°
longhorn-system              1200m       211m    989m     82.4%  üü°
```

### 2. An√°lise Profunda - Entendendo o Porqu√™

Para cada namespace com alto desperd√≠cio, fui a fundo:

**a) Identifica√ß√£o dos workloads:**
```bash
kubectl get pods -n velero -o custom-columns='NAME:.metadata.name,CPU_REQ:.spec.containers[*].resources.requests.cpu'
```

**Output:**
```
NAME                     CPU_REQ
node-agent-xxx (3x)      20m cada
velero-xxx               500m
```

**b) Correla√ß√£o com uso real:**
```bash
kubectl top pods -n velero
```

**Output:**
```
NAME                     CPU(cores)   
node-agent-xxx           1m          ‚Üê Pediu 20m, usa 1m (95% desperd√≠cio)
velero-xxx               5m          ‚Üê Pediu 500m, usa 5m (99% desperd√≠cio)
```

**c) Valida√ß√£o com m√©tricas hist√≥ricas (Prometheus/Grafana):**
- An√°lise de 7 dias de hist√≥rico
- Identifica√ß√£o de picos de uso
- C√°lculo de P95/P99 para definir requests adequados

### 3. Implementa√ß√£o - Aplicando as mudan√ßas com Seguran√ßa

Minha estrat√©gia foi:
- Abordagem gradual (um namespace por vez, nunca tudo de uma vez)
- Testar primeiro no cluster AMP 
- Rolling updates para garantir zero downtime
- Manter margem de seguran√ßa: requests = uso_pico √ó 1.5-2.0

Os comandos que apliquei no Velero:

```bash
# DaemonSet node-agent: 20m ‚Üí 5m (uso real: ~1m)
kubectl patch daemonset node-agent -n velero --type='json' \
  -p='[{"op": "replace", "path": "/spec/template/spec/containers/0/resources/requests/cpu", "value": "5m"}]'

# Deployment velero: 500m ‚Üí 30m (uso real: ~5m)
kubectl patch deployment velero -n velero --type='json' \
  -p='[{"op": "replace", "path": "/spec/template/spec/containers/0/resources/requests/cpu", "value": "30m"}]'
```

### 4. Valida√ß√£o - Confirmando que Deu Certo

Depois de aplicar as mudan√ßas, fiz uma valida√ß√£o bem detalhada:

**Verifica√ß√£o de rollout:**
```bash
kubectl rollout status deployment/velero -n velero
# deployment "velero" successfully rolled out ‚úÖ
```

**b) Confirma√ß√£o de novos valores:**
```bash
kubectl get pods -n velero -o custom-columns='NAME:.metadata.name,CPU_REQ:.spec.containers[*].resources.requests.cpu'

# Output:
NAME                     CPU_REQ
node-agent-xxx           5m      ‚úÖ (antes: 20m)
velero-xxx               30m     ‚úÖ (antes: 500m)
```

**c) Monitoramento p√≥s-mudan√ßa:**
- ‚úÖ Pods rodando normalmente
- ‚úÖ Sem OOMKilled ou CPU throttling
- ‚úÖ Lat√™ncia e performance sem altera√ß√£o
- ‚úÖ Backups continuam funcionando

---

## Ferramentas que Usei

- **Kubernetes:** RKE2 (Rancher Kubernetes Engine 2)
- **Orquestra√ß√£o:** Rancher
- **Metrics:** rke2-metrics-server
- **Monitoramento:** Prometheus + Grafana
- **Backup:** Velero
- **Service Mesh:** Istio
- **Storage:** Longhorn
- **Scripts:** Bash + jq + kubectl

---

## O Que Criei (Artefatos)

### Scripts de Auditoria

- **`check_slack_percent.sh`**: Calcula desperd√≠cio por namespace
- **`diagnostico_metrics_rke2.sh`**: Valida funcionamento do metrics-server
- **`correcao_<namespace>.sh`**: Scripts automatizados de corre√ß√£o

### 2. Documenta√ß√£o T√©cnica

- **`otimizacao-kubernetes.md`**: Documenta√ß√£o completo com mais de 70 p√°ginas
  - Metodologia de diagn√≥stico
  - Comandos de corre√ß√£o
  - Casos reais com antes/depois
  - Troubleshooting
  - Boas pr√°ticas

### 3. Processos Estabelecidos

- ‚úÖ Auditoria semanal automatizada (cron job)
- ‚úÖ Checklist de valida√ß√£o pr√©/p√≥s mudan√ßa
- ‚úÖ Documenta√ß√£o de decis√µes t√©cnicas
- ‚úÖ Integra√ß√£o com GitOps

---

## O que aprendi durante o processo

### Coisas que funcionaram bem

1. **Abordagem Data-Driven**: Decis√µes baseadas em m√©tricas reais (Prometheus) e n√£o em "achismos"
2. **Itera√ß√£o Gradual**: Come√ßar com namespace menos cr√≠tico (velero) reduziu riscos
3. **Automa√ß√£o**: Scripts reutiliz√°veis aceleram an√°lise de outros namespaces
4. **Margem de Seguran√ßa**: Manter requests 5-7x maiores que uso real evitou problemas

### Desafios que enfrentei

1. **Metrics-server RKE2**: Naming diferente (`rke2-metrics-server` vs `metrics-server`)
2. **Parsing de Dados**: Necessidade de tratar formatos mistos (millicores "m" vs cores inteiros)
3. **Sidecars Istio**: Descobrir que grande parte do desperd√≠cio vinha dos proxies

### Pr√≥ximos Passos

| Namespace | Potencial de Economia | Status |
|-----------|----------------------|--------|
| velero | 515m | ‚úÖ Conclu√≠do |
| istio-system | ~1350m | üîÑ Planejado |
| cattle-monitoring | ~750m | üîÑ Planejado |
| kube-system | ~2500m | üîÑ Em an√°lise |
| longhorn-system | ~850m | üîÑ Planejado |


---

## Compet√™ncias que apliquei (SRE)

### Habilidades T√©cnicas
- Observabilidade: Prometheus, Grafana, metrics-server
- Kubernetes Avan√ßado: Resource management, scheduling, QoS
- Automa√ß√£o: Bash scripting, jq, kubectl
- Troubleshooting: Diagn√≥stico sistem√°tico de problemas complexos

### Pr√°ticas SRE
- Capacity Planning: An√°lise de tend√™ncias e proje√ß√µes
- Cost Optimization: Redu√ß√£o de desperd√≠cio sem impacto em SLA
- Toil Reduction: Automa√ß√£o de auditorias e corre√ß√µes
- Documentation: Playbooks, runbooks e conhecimento compartilhado

### Soft Skills
- Iniciativa: Identifica√ß√£o proativa de problema n√£o mapeado
- Pensamento Anal√≠tico: Decomposi√ß√£o de problema complexo
- Comunica√ß√£o T√©cnica: Documenta√ß√£o clara e objetiva
- Risk Management: Abordagem gradual e revers√≠vel

---

## Como voc√™ pode usar esse projeto

### O que voc√™ precisa

```bash
# Ferramentas necess√°rias
- kubectl configurado
- jq instalado
- Acesso admin ao cluster
- metrics-server funcional
```

### Passo a Passo para aplica no seu Cluster

1. **Clone este reposit√≥rio**
```bash
git clone https://github.com/seu-usuario/k8s-resource-optimization
cd k8s-resource-optimization
```

2. **Execute diagn√≥stico**
```bash
chmod +x diagnostico_metrics_rke2.sh
./diagnostico_metrics_rke2.sh
```

3. **Execute auditoria**
```bash
chmod +x check_slack_percent.sh
./check_slack_percent.sh > auditoria_$(date +%Y%m%d).txt
```

4. **Analise resultados e priorize namespaces**

5. **Para cada namespace:**
   - Analise requests vs uso real
   - Consulte m√©tricas hist√≥ricas
   - Calcule novo request: `uso_pico √ó 1.5`
   - Aplique patch gradualmente
   - Valide e monitore

### Estrutura do Reposit√≥rio

```
k8s-resource-optimization/
‚îú‚îÄ‚îÄ README.md                          # Este arquivo
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îú‚îÄ‚îÄ otimizacao-kubernetes.md      # Documenta√ß√£o completa
‚îÇ   ‚îî‚îÄ‚îÄ caso-velero.md                # Case detalhado
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ check_slack_percent.sh        # Auditoria principal
‚îÇ   ‚îú‚îÄ‚îÄ diagnostico_metrics_rke2.sh   # Diagn√≥stico
‚îÇ   ‚îî‚îÄ‚îÄ correcao_velero.sh            # Exemplo de corre√ß√£o
‚îî‚îÄ‚îÄ exemplos/
    ‚îú‚îÄ‚îÄ auditoria_20260131.txt        # Output real
    ‚îî‚îÄ‚îÄ grafana_screenshots/          # Evid√™ncias
```

---

## Quer Contribuir?

Este projeto √© open-source! Se voc√™ quiser ajudar:

- Reporte bugs ou problemas que encontrar
- Sugira melhorias nos scripts
- Ajude a melhorar a documenta√ß√£o
- D√™ uma star se achou √∫til!

---

## Contato

**Marcelo Loiola**  
Software Architect | DevOps Engineer | Cloud Engineer

- **Email:** marceloloiola.ti@gmail.com
- **WhatsApp:** +55 61 98408-6866
- **LinkedIn:** [linkedin.com/in/marcelo-loiola](https://linkedin.com/in/marcelo-loiola)
- **GitHub:** [github.com/marceloloiola](https://github.com/marceloloiola)

---

## Licen√ßa

Este projeto est√° sob a licen√ßa MIT. Sinta-se livre para usar, modificar e distribuir.

---

## Agradecimentos

Ferramentas e projetos que me inspiraram:
- [Vertical Pod Autoscaler](https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler)
- [Goldilocks](https://github.com/FairwindsOps/goldilocks)
- [Kube-resource-report](https://github.com/hjacobs/kube-resource-report)

---

**"Otimizar n√£o √© sobre cortar recursos, √© sobre usar recursos de forma inteligente."**
