# Playbook: Otimiza√ß√£o de Recursos em Kubernetes (Right-Sizing)

Este documento descreve o processo de identifica√ß√£o, an√°lise e corre√ß√£o de aloca√ß√£o excessiva de recursos (CPU/Mem√≥ria) em clusters Kubernetes. O objetivo √© reduzir o **Slack** (diferen√ßa entre o que foi reservado e o que √© realmente usado).

**Vers√£o atualizada e testada em ambiente RKE2/Rancher**

---

## üìã √çndice

1. [Diagn√≥stico: Identificando o Desperd√≠cio](#1-diagn√≥stico-identificando-o-desperd√≠cio-global)
2. [Investiga√ß√£o: Encontrando o Pod "Gordo"](#2-investiga√ß√£o-encontrando-o-pod-gordo)
3. [Corre√ß√£o: Aplicando o Right-Sizing](#3-corre√ß√£o-aplicando-o-right-sizing)
4. [Valida√ß√£o](#4-valida√ß√£o)
5. [An√°lise de Desperd√≠cio por Namespace](#5-an√°lise-de-desperd√≠cio-por-namespace)
6. [Caso Real: Otimiza√ß√£o do Velero](#6-caso-real-otimiza√ß√£o-do-velero)
7. [Cheat Sheet](#7-cheat-sheet-comandos-r√°pidos)
8. [Boas Pr√°ticas](#8-boas-pr√°ticas)
9. [Troubleshooting](#9-troubleshooting)

---

## 1. Diagn√≥stico: Identificando o Desperd√≠cio (Global)

O primeiro passo √© ter uma vis√£o macro de quais **Namespaces** s√£o os maiores ofensores. Para isso, utilizamos um script que compara a soma dos `requests` com a soma do uso real (`top`).

### Pr√©-requisito: Metrics Server

Antes de executar os scripts de auditoria, verifique se o metrics-server est√° funcionando:

```bash
# Para RKE2
./diagnostico_metrics_rke2.sh

# Teste r√°pido
kubectl top nodes
```

Se o comando `kubectl top nodes` funcionar, voc√™ est√° pronto para executar a auditoria!

### Script de Auditoria de Slack (CPU) com Percentual

O script `check_slack_percent.sh` √© a ferramenta principal para identificar desperd√≠cio:

```bash
# Executar auditoria completa
chmod +x check_slack_percent.sh
./check_slack_percent.sh

# Ver apenas os top 10 desperdi√ßadores
./check_slack_percent.sh | head -12

# Salvar resultado em arquivo
./check_slack_percent.sh > auditoria_$(date +%Y%m%d).txt
```

### Interpreta√ß√£o dos Resultados

**Exemplo de Output Real:**

```
NAMESPACE                           REQUESTED       USED            SLACK (m)       WASTE %
---------------------------------------------------------------------------------------------------
kube-system                         3925m           625m            3300m           84.1%
istio-system                        1410m           21m             1389m           98.5%
longhorn-system                     1200m           211m            989m            82.4%
velero                              560m            6m              554m            98.9%
cattle-monitoring-system            950m            111m            839m            88.3%
```

**Crit√©rios de Prioriza√ß√£o:**

- üî¥ **CR√çTICO** (Waste > 95%): Ajuste imediato
- üü° **ALTO** (Waste 80-95%): Ajuste em 1 semana
- üü¢ **MODERADO** (Waste 60-80%): Monitorar e ajustar
- ‚úÖ **SAUD√ÅVEL** (Waste < 60%): Manter

---

## 2. Investiga√ß√£o: Encontrando o Pod "Gordo"

Ap√≥s identificar o namespace problem√°tico, precisamos descobrir qual carga de trabalho (Workload) est√° superdimensionada.

### Listar Requests por Pod

```bash
kubectl get pods -n <NAMESPACE> -o custom-columns='NAME:.metadata.name,CPU_REQ:.spec.containers[*].resources.requests.cpu,MEM_REQ:.spec.containers[*].resources.requests.memory'
```

**Exemplo Real (Velero):**

```
NAME                     CPU_REQ   MEM_REQ
node-agent-4mc7m         20m       128Mi
node-agent-bxpd4         20m       128Mi
node-agent-zfzx9         20m       128Mi
velero-b655f5996-jfsfv   500m      128Mi
```

### Analisar Uso Real

```bash
kubectl top pods -n <NAMESPACE>
```

**Exemplo Real (Velero):**

```
NAME                     CPU(cores)   MEMORY(bytes)
node-agent-4mc7m         1m           28Mi
node-agent-bxpd4         1m           27Mi
node-agent-zfzx9         1m           26Mi
velero-b655f5996-jfsfv   5m           245Mi
```

### Analisar Containers Individuais (App vs. Sidecar)

Muitas vezes o "vil√£o" n√£o √© a aplica√ß√£o, mas o sidecar (ex: `istio-proxy`). O comando abaixo detalha o request de cada container dentro do pod:

```bash
kubectl get pod <NOME_DO_POD> -n <NAMESPACE> -o jsonpath='{range .spec.containers[*]}{.name}{"\t"}{.resources.requests.cpu}{"\t"}{.resources.requests.memory}{"\n"}{end}'
```

**Exemplo de Output (Desperd√≠cio com Istio):**

```
minha-app      20m    256Mi  <-- √ìtimo
istio-proxy    100m   128Mi  <-- Vil√£o (Default alto para ambiente n√£o-prod)
```

### Verificar Uso Real vs Solicitado

```bash
# Ver uso atual detalhado
kubectl top pod <NOME_DO_POD> -n <NAMESPACE> --containers

# Ver requests e limits configurados
kubectl describe pod <NOME_DO_POD> -n <NAMESPACE> | grep -A 5 "Requests"
```

---

## 3. Corre√ß√£o: Aplicando o Right-Sizing

Para corrigir, podemos aplicar patches diretamente no cluster (solu√ß√£o imediata) ou ajustar os manifestos no Git (solu√ß√£o definitiva/GitOps).

### A. Reduzindo a Aplica√ß√£o (Container Principal)

Se a aplica√ß√£o pede muito (ex: 200m) e usa pouco, reduzimos o request.

**M√©todo Seguro (Por Posi√ß√£o - Index 0):** Usa-se quando n√£o temos certeza do nome do container.

```bash
# Para StatefulSet
kubectl patch sts <NOME_DO_STS> -n <NAMESPACE> --type='json' \
  -p='[{"op": "replace", "path": "/spec/template/spec/containers/0/resources/requests/cpu", "value": "10m"}]'

# Para Deployment
kubectl patch deployment <NOME_DO_DEPLOY> -n <NAMESPACE> --type='json' \
  -p='[{"op": "replace", "path": "/spec/template/spec/containers/0/resources/requests/cpu", "value": "10m"}]'

# Para DaemonSet
kubectl patch daemonset <NOME_DO_DS> -n <NAMESPACE> --type='json' \
  -p='[{"op": "replace", "path": "/spec/template/spec/containers/0/resources/requests/cpu", "value": "10m"}]'
```

**Para Mem√≥ria:**

```bash
kubectl patch deployment <NOME> -n <NAMESPACE> --type='json' \
  -p='[{"op": "replace", "path": "/spec/template/spec/containers/0/resources/requests/memory", "value": "128Mi"}]'
```

### B. Reduzindo o Sidecar (Istio)

Para ambientes de Dev/Homologa√ß√£o, o default de 100m do Istio √© excessivo. Usamos uma **Annotation** para sobrescrever esse valor por Pod.

**Comando de Patch (Injection):**

```bash
# Para StatefulSet
kubectl patch sts <NOME_DO_STS> -n <NAMESPACE> --type='merge' \
  -p '{"spec": {"template": {"metadata": {"annotations": {"sidecar.istio.io/proxyCPU": "10m", "sidecar.istio.io/proxyMemory": "50Mi"}}}}}'

# Para Deployment
kubectl patch deployment <NOME_DO_DEPLOY> -n <NAMESPACE> --type='merge' \
  -p '{"spec": {"template": {"metadata": {"annotations": {"sidecar.istio.io/proxyCPU": "10m", "sidecar.istio.io/proxyMemory": "50Mi"}}}}}'
```

### C. Script Automatizado de Corre√ß√£o

Para facilitar, crie scripts espec√≠ficos por namespace. Exemplo:

```bash
#!/bin/bash
# correcao_<namespace>.sh

# Identificar workloads
DEPLOYMENT=$(kubectl get deployment -n <namespace> -o jsonpath='{.items[0].metadata.name}')

# Aplicar corre√ß√£o
kubectl patch deployment $DEPLOYMENT -n <namespace> --type='json' \
  -p='[{"op": "replace", "path": "/spec/template/spec/containers/0/resources/requests/cpu", "value": "30m"}]'

# Validar
echo "Aguardando rollout..."
kubectl rollout status deployment/$DEPLOYMENT -n <namespace>

echo "Novos valores:"
kubectl get pods -n <namespace> -o custom-columns='NAME:.metadata.name,CPU_REQ:.spec.containers[*].resources.requests.cpu'
```

---

## 4. Valida√ß√£o

Ap√≥s a aplica√ß√£o dos patches, os Pods reiniciar√£o. Valide se a altera√ß√£o surtiu efeito:

### 1. Aguarde o status `Running`:

```bash
kubectl get pods -n <NAMESPACE> -w
```

### 2. Verifique os novos valores alocados:

```bash
kubectl get pods -n <NAMESPACE> -o custom-columns='NAME:.metadata.name,CPU_REQ:.spec.containers[*].resources.requests.cpu,MEM_REQ:.spec.containers[*].resources.requests.memory'
```

**Resultado Esperado (Exemplo Velero):**

```
NAME                     CPU_REQ   MEM_REQ
node-agent-4xhcx         5m        128Mi     ‚úÖ (antes: 20m)
node-agent-7lq5p         5m        128Mi     ‚úÖ (antes: 20m)
node-agent-zhkv2         5m        128Mi     ‚úÖ (antes: 20m)
velero-8766b5d9d-2rvcn   30m       128Mi     ‚úÖ (antes: 500m)
```

### 3. Monitore o uso real ap√≥s a mudan√ßa:

```bash
kubectl top pods -n <NAMESPACE>
```

### 4. Execute novamente o script de auditoria:

```bash
./check_slack_percent.sh | grep <NAMESPACE>
```

**Compara√ß√£o Antes/Depois (Exemplo Velero):**

```
ANTES:  velero    560m    6m    554m    98.9%
DEPOIS: velero    45m     6m    39m     86.7%
```

---

## 5. An√°lise de Desperd√≠cio por Namespace

### Identificando Namespaces com Maior Desperd√≠cio

Execute o script de auditoria e analise os resultados focando em:

1. **Alto Slack Absoluto**: Namespaces desperdi√ßando mais recursos em termos totais
2. **Alto Percentual de Desperd√≠cio**: `(SLACK / REQUESTED) * 100`

### Script Completo

O script `check_slack_percent.sh` j√° calcula ambas as m√©tricas automaticamente.

```bash
./check_slack_percent.sh
```

### Crit√©rios de Prioriza√ß√£o

Priorize a otimiza√ß√£o de namespaces que atendam a um ou mais crit√©rios:

- **Slack > 1000m** (1 CPU completo desperdi√ßado)
- **Desperd√≠cio > 80%** do solicitado
- **Namespaces de desenvolvimento/homologa√ß√£o** (geralmente sobre-provisionados)
- **Namespaces cr√≠ticos com overcommit** no node

### Exemplo de Prioriza√ß√£o Real

Com base em auditoria real:

| Prioridade | Namespace | Slack | Waste % | A√ß√£o |
|------------|-----------|-------|---------|------|
| 1 | velero | 554m | 98.9% | ‚úÖ Corrigido |
| 2 | istio-system | 1389m | 98.5% | üîÑ Pr√≥ximo |
| 3 | cattle-monitoring-system | 839m | 88.3% | üîÑ Pr√≥ximo |
| 4 | kube-system | 3300m | 84.1% | üîÑ Pr√≥ximo |
| 5 | longhorn-system | 989m | 82.4% | üîÑ Pr√≥ximo |

---

## 6. Caso Real: Otimiza√ß√£o do Velero

### Situa√ß√£o Inicial

**Namespace:** velero  
**Desperd√≠cio:** 98.9% (554m de 560m solicitados)

**Pods Identificados:**
```
NAME                     CPU_REQ   USED
node-agent (3x)         20m       ~0.5m cada
velero                  500m      ~5m
```

### An√°lise com Grafana/Prometheus

Gr√°ficos mostraram:
- **node-agent**: Uso consistente de 0.0005 cpu (~0.5m)
- **velero**: Uso m√©dio de 0.005 cpu (~5m) com picos em 0.015 cpu (~15m)

### Corre√ß√£o Aplicada

**Script Criado: `correcao_velero.sh`**

```bash
#!/bin/bash
# Otimiza√ß√£o Velero

echo "üöÄ Aplicando otimiza√ß√£o no namespace velero..."

# Corrigir DaemonSet node-agent (20m ‚Üí 5m)
kubectl patch daemonset node-agent -n velero --type='json' \
  -p='[{"op": "replace", "path": "/spec/template/spec/containers/0/resources/requests/cpu", "value": "5m"}]'

# Corrigir Deployment velero (500m ‚Üí 30m)
kubectl patch deployment velero -n velero --type='json' \
  -p='[{"op": "replace", "path": "/spec/template/spec/containers/0/resources/requests/cpu", "value": "30m"}]'

echo "‚úÖ Corre√ß√µes aplicadas!"
```

**Execu√ß√£o:**

```bash
chmod +x correcao_velero.sh
./correcao_velero.sh
```

### Resultado

| M√©trica | Antes | Depois | Economia |
|---------|-------|--------|----------|
| node-agent (3x) | 60m | 15m | 75% |
| velero | 500m | 30m | 94% |
| **TOTAL** | **560m** | **45m** | **92%** |

**Impacto no Cluster:**
- 515m de CPU liberados
- Redu√ß√£o de desperd√≠cio de 98.9% para 86.7%
- Sem impacto na performance (requests ainda 5x maiores que uso real)

---

## 7. Cheat Sheet (Comandos R√°pidos)

| A√ß√£o | Comando |
|------|---------|
| Ver Top CPU (Node) | `kubectl top nodes` |
| Ver Top CPU (Pod) | `kubectl top pods -n <ns>` |
| Ver Top CPU (Pod espec√≠fico) | `kubectl top pod <nome-pod> -n <ns>` |
| Ver Top com containers | `kubectl top pod <nome> -n <ns> --containers` |
| Listar Deployments | `kubectl get deploy -n <ns>` |
| Listar StatefulSets | `kubectl get sts -n <ns>` |
| Listar DaemonSets | `kubectl get ds -n <ns>` |
| Listar Rollouts | `kubectl get rollout -n <ns>` |
| Ver requests/limits | `kubectl get pods -n <ns> -o custom-columns='NAME:.metadata.name,CPU_REQ:.spec.containers[*].resources.requests.cpu,MEM_REQ:.spec.containers[*].resources.requests.memory'` |
| Verificar Logs (Erro) | `kubectl logs <pod> -c <container> --previous` |
| Ver eventos do namespace | `kubectl get events -n <ns> --sort-by='.lastTimestamp'` |
| Descrever pod | `kubectl describe pod <nome> -n <ns>` |
| Ver todos os recursos | `kubectl get all -n <ns>` |
| Verificar rollout status | `kubectl rollout status deployment/<nome> -n <ns>` |
| Reverter rollout | `kubectl rollout undo deployment/<nome> -n <ns>` |
| Ver hist√≥rico de rollout | `kubectl rollout history deployment/<nome> -n <ns>` |

---

## 8. Boas Pr√°ticas

### Definindo Requests Adequados

1. **Monitore o uso real** por pelo menos 1 semana antes de ajustar
2. **Use m√©tricas de pico**, n√£o apenas m√©dias
3. **Adicione margem de seguran√ßa**: 20-50% acima do uso de pico
4. **Teste em ambientes n√£o-produtivos** primeiro

**F√≥rmula Recomendada:**

```
Request Ideal = (Uso de Pico √ó 1.3) + Buffer de Burst
```

**Exemplo Real (Velero):**
- Uso m√©dio: 5m
- Uso de pico: 15m
- Request recomendado: 15m √ó 1.5 = 22.5m ‚Üí **30m** ‚úÖ

### Diferen√ßa entre Requests e Limits

- **Requests**: Recursos **garantidos** para o pod (afeta scheduling)
- **Limits**: Recursos **m√°ximos** que o pod pode usar (afeta throttling)

**Recomenda√ß√£o por Tipo de Aplica√ß√£o:**

| Tipo | Requests | Limits |
|------|----------|--------|
| **Aplica√ß√µes est√°veis** | Uso pico √ó 1.3 | Requests √ó 1.5 |
| **Aplica√ß√µes com burst** | Uso m√©dio √ó 1.5 | Requests √ó 3 |
| **Aplica√ß√µes cr√≠ticas** | Uso pico √ó 1.5 | Requests √ó 2 |
| **Jobs/CronJobs** | Uso hist√≥rico | Requests √ó 2 |

### Ambientes Dev/Hml vs Produ√ß√£o

**Desenvolvimento/Homologa√ß√£o:**
- Requests mais baixos (recursos limitados)
- Limits mais agressivos
- Toler√¢ncia a throttling maior

**Produ√ß√£o:**
- Requests generosos (garantir QoS)
- Limits com margem confort√°vel
- Priorizar disponibilidade

### Monitoramento Cont√≠nuo

Execute o script de auditoria periodicamente:

```bash
# Criar cron job para executar semanalmente
crontab -e

# Adicionar linha:
0 9 * * 1 /path/to/check_slack_percent.sh > /var/log/k8s-slack-report-$(date +\%Y\%m\%d).log
```

### Documenta√ß√£o de Mudan√ßas

Mantenha um log de todas as otimiza√ß√µes:

```bash
# Criar arquivo de log
echo "$(date) - velero: 560m ‚Üí 45m (92% economia)" >> otimizacoes.log
```

---

## 9. Troubleshooting

### Pod n√£o inicia ap√≥s redu√ß√£o de recursos

**Sintomas:**
- Pod fica em `Pending`
- Pod entra em `CrashLoopBackOff`
- Eventos mostram `Insufficient cpu` ou `Insufficient memory`

**Diagn√≥stico:**

```bash
# Verificar eventos
kubectl describe pod <nome> -n <ns>

# Ver logs
kubectl logs <nome> -n <ns>

# Ver eventos do namespace
kubectl get events -n <ns> --sort-by='.lastTimestamp' | tail -20
```

**Solu√ß√£o:**

```bash
# Se necess√°rio, reverter
kubectl rollout undo deployment/<nome> -n <ns>

# Ou aumentar gradualmente
kubectl patch deployment <nome> -n <ns> --type='json' \
  -p='[{"op": "replace", "path": "/spec/template/spec/containers/0/resources/requests/cpu", "value": "50m"}]'
```

### OOMKilled (Out of Memory)

**Sintomas:**
- Pod morto com status `OOMKilled`
- Logs mostram `Killed` ou `signal 9`

**Diagn√≥stico:**

```bash
# Verificar hist√≥rico
kubectl describe pod <nome> -n <ns> | grep -A 5 "Last State"

# Ver uso de mem√≥ria antes do kill
kubectl top pod <nome> -n <ns> --containers
```

**Solu√ß√£o:**

Se um pod for morto por falta de mem√≥ria ap√≥s ajuste:

```bash
# Aumentar gradualmente (exemplo: 128Mi ‚Üí 256Mi ‚Üí 512Mi)
kubectl patch deployment <nome> -n <ns> --type='json' \
  -p='[{"op": "replace", "path": "/spec/template/spec/containers/0/resources/requests/memory", "value": "256Mi"}]'

# Tamb√©m aumentar o limit
kubectl patch deployment <nome> -n <ns> --type='json' \
  -p='[{"op": "replace", "path": "/spec/template/spec/containers/0/resources/limits/memory", "value": "512Mi"}]'
```

### CPU Throttling Excessivo

**Sintomas:**
- Aplica√ß√£o lenta
- Lat√™ncia aumentada
- Logs mostram timeouts

**Diagn√≥stico:**

```bash
# No Prometheus/Grafana, buscar por:
# container_cpu_cfs_throttled_seconds_total

# Ou via kubectl (metrics-server n√£o mostra throttling)
kubectl describe pod <nome> -n <ns> | grep -i throttl
```

**Solu√ß√£o:**

```bash
# Aumentar requests E limits
kubectl patch deployment <nome> -n <ns> --type='json' \
  -p='[
    {"op": "replace", "path": "/spec/template/spec/containers/0/resources/requests/cpu", "value": "100m"},
    {"op": "replace", "path": "/spec/template/spec/containers/0/resources/limits/cpu", "value": "200m"}
  ]'
```

### Pods n√£o distribuem uniformemente

**Sintomas:**
- Um node est√° cheio enquanto outros est√£o vazios
- Novos pods ficam `Pending`

**Diagn√≥stico:**

```bash
# Ver distribui√ß√£o de pods por node
kubectl get pods -A -o wide | awk '{print $8}' | sort | uniq -c

# Ver recursos alocados por node
kubectl describe nodes | grep -A 5 "Allocated resources"
```

**Solu√ß√£o:**

```bash
# Usar Pod Anti-Affinity ou Topology Spread Constraints
# Ou instalar o Descheduler do Kubernetes
```

### Metrics Server n√£o funciona

**Sintomas:**
- `kubectl top` retorna erro
- Scripts de auditoria mostram `USED = 0m`

**Diagn√≥stico:**

```bash
./diagnostico_metrics_rke2.sh
```

**Solu√ß√£o:**

Ver se√ß√£o de troubleshooting do metrics-server no script de diagn√≥stico.

---

## 10. Pr√≥ximos Passos

### Roadmap de Otimiza√ß√£o

Baseado em auditoria real, sugerimos esta ordem:

1. ‚úÖ **velero** (560m ‚Üí 45m) - CONCLU√çDO
2. üîÑ **istio-system** (1410m ‚Üí ~50m) - Em an√°lise
3. üîÑ **cattle-monitoring-system** (950m ‚Üí ~200m) - Em an√°lise
4. üîÑ **kube-system** (3925m ‚Üí ~800m) - Requer cuidado extra
5. üîÑ **longhorn-system** (1200m ‚Üí ~300m) - Em an√°lise

### Ferramentas Complementares

- **Vertical Pod Autoscaler (VPA)**: Recomenda√ß√µes autom√°ticas
- **Goldilocks**: Dashboard para recomenda√ß√µes de resources
- **Kube-resource-report**: Relat√≥rios de utiliza√ß√£o
- **Prometheus + Grafana**: Monitoramento de longo prazo

### Integra√ß√£o com GitOps

Ap√≥s validar as mudan√ßas no cluster, atualize os manifestos:

```yaml
# deployment.yaml
spec:
  template:
    spec:
      containers:
      - name: app
        resources:
          requests:
            cpu: 30m      # ‚úÖ Atualizado ap√≥s auditoria
            memory: 128Mi
          limits:
            cpu: 100m
            memory: 256Mi
```

---

## 11. Refer√™ncias

- [Kubernetes Resource Management](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/)
- [Istio Sidecar Resource Annotations](https://istio.io/latest/docs/reference/config/annotations/)
- [kubectl Cheat Sheet](https://kubernetes.io/docs/reference/kubectl/cheatsheet/)
- [Vertical Pod Autoscaler](https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler)
- [RKE2 Documentation](https://docs.rke2.io/)
- [Rancher Documentation](https://rancher.com/docs/)

---

**√öltima atualiza√ß√£o:** Janeiro 2026  
**Ambiente testado:** RKE2 + Rancher  
**Vers√£o:** 2.0 (com casos reais e valida√ß√£o)
